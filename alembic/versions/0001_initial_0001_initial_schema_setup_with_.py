"""0001 initial schema setup with TimescaleDB and composite PK

Revision ID: 0001_initial
Revises: 
Create Date: 2025-05-05 08:30:48.296482

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0001_initial'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('users',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('email', sa.String(length=255), nullable=False),
    sa.Column('password_hash', sa.String(length=255), nullable=False),
    sa.Column('tier', sa.String(length=20), nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_users')),
    sa.UniqueConstraint('email', name=op.f('uq_users_email'))
    )
    op.create_table('log_entries',
    sa.Column('id', sa.BigInteger(), nullable=False),
    sa.Column('user_id', sa.UUID(), nullable=False),
    sa.Column('correlation_id', sa.UUID(), nullable=True),
    sa.Column('event_type', sa.String(length=64), nullable=False),
    sa.Column('service_origin', sa.String(length=64), nullable=True),
    sa.Column('payload', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('log_timestamp', postgresql.TIMESTAMP(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('fk_log_entries_user_id_users'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', 'log_timestamp', name='pk_log_entries')
    )
    with op.batch_alter_table('log_entries', schema=None) as batch_op:
        batch_op.create_index('ix_log_entries_event_type_log_timestamp_desc', ['event_type', sa.literal_column('log_timestamp DESC')], unique=False)
        batch_op.create_index('ix_log_entries_user_id_log_timestamp_desc', ['user_id', sa.literal_column('log_timestamp DESC')], unique=False)

    op.create_table('profiles',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('user_id', sa.UUID(), nullable=False),
    sa.Column('birth_datetime', postgresql.TIMESTAMP(timezone=True), nullable=False),
    sa.Column('birth_lat', sa.Numeric(precision=9, scale=6), nullable=False),
    sa.Column('birth_lon', sa.Numeric(precision=9, scale=6), nullable=False),
    sa.Column('birth_tz', sa.String(length=64), nullable=False),
    sa.Column('hd_data', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('astro_data', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('synthesis_results', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('fk_profiles_user_id_users'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_profiles'))
    )
    op.create_table('typology_answers',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('user_id', sa.UUID(), nullable=False),
    sa.Column('question_id', sa.String(length=64), nullable=False),
    sa.Column('answer_value', sa.String(length=64), nullable=False),
    sa.Column('answered_at', postgresql.TIMESTAMP(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], name=op.f('fk_typology_answers_user_id_users'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_typology_answers')),
    sa.UniqueConstraint('user_id', 'question_id', name='uq_typology_answers_user_question')
    )
    op.create_table('typology_results',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('typology_name', sa.String(length=64), nullable=False),
    sa.Column('confidence', sa.Numeric(precision=3, scale=2), nullable=False),
    sa.Column('raw_vector', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
    sa.Column('generated_at', postgresql.TIMESTAMP(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['id'], ['profiles.id'], name=op.f('fk_typology_results_id_profiles'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_typology_results'))
    )

    # --- TimescaleDB Setup ---
    op.execute("CREATE EXTENSION IF NOT EXISTS timescaledb;")
    op.execute(
        """
        SELECT create_hypertable(
            'log_entries',
            'log_timestamp',
            if_not_exists => TRUE
        );
        """
    )
    # Add compression (adjust segmentby/orderby as needed)
    op.execute(
        """
        ALTER TABLE log_entries SET (
            timescaledb.compress,
            timescaledb.compress_segmentby = 'id, event_type', -- Example: segment by id and event_type
            timescaledb.compress_orderby = 'log_timestamp DESC'
        );
        """
    )
    # Add compression policy (e.g., compress chunks older than 30 days)
    op.execute(
        """
        SELECT add_compression_policy('log_entries', INTERVAL '30 days');
        """
    )
    # --- End TimescaleDB Setup ---

    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###

    # --- TimescaleDB Downgrade ---
    # Note: Dropping the extension implicitly drops hypertables, policies etc.
    # If more granular control is needed, add specific DROP commands before this.
    op.execute("DROP EXTENSION IF EXISTS timescaledb;")
    # --- End TimescaleDB Downgrade ---

    # Drop application tables (reverse order of creation)
    op.drop_table('typology_results')
    op.drop_table('typology_answers')
    op.drop_table('profiles')
    with op.batch_alter_table('log_entries', schema=None) as batch_op:
        batch_op.drop_index('ix_log_entries_user_id_log_timestamp_desc')
        batch_op.drop_index('ix_log_entries_event_type_log_timestamp_desc')

    op.drop_table('log_entries')
    op.drop_table('users')
    # ### end Alembic commands ###
